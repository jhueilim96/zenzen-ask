{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General API call flow\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Producer = Prepare API request\n",
    "   1. Get template from folder\n",
    "   2. Get ANIME seed from seed folder\n",
    "   3. One connecton per producer task\n",
    "      1. Producer loop through page\n",
    "      2. Wait result to arive\n",
    "      3. check `hasNextPage=True`\n",
    "      4. Repeat\n",
    "   4. Render variables\n",
    "      1. perPage = 50 (max allowed)\n",
    "   5. Create mark source and target \n",
    "      1. To enable self healing feature\n",
    "2. Consumer = Response parser and storage\n",
    "   1. Fetch data from Queue\n",
    "   2. Parse `data` key\n",
    "   3. Set storage destinationjson -> 1 json per api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import duckdb \n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def make_api_call(url:str, query:str, variables:dict):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url, json={'query': query, 'variables': variables}) as resp:\n",
    "            result = await resp.json()\n",
    "            header = resp.headers\n",
    "            status_code = resp.status\n",
    "            return result, resp\n",
    "        \n",
    "        \n",
    "async def producer(queue, ids:list[int], url:str, query:str, duration_min:float):\n",
    "    print(\"Producer: Started\")\n",
    "    remaining_request = 90\n",
    "    for id in ids:\n",
    "        start_time = time.time()\n",
    "        variables = {  \n",
    "            'media_id': id\n",
    "        }\n",
    "\n",
    "        print(f\"Requesting page for {id}\")\n",
    "        result, resp = await make_api_call(url, query, variables)            \n",
    "        header = resp.headers\n",
    "\n",
    "        if resp.status != 200:\n",
    "            print(f\"Request fails at {id}\")\n",
    "            raise resp.raise_for_status()\n",
    "\n",
    "        await queue.put((result, id))\n",
    "        remaining_request = int(header.get('x-ratelimit-remaining'))\n",
    "        print(f'Remaining API call per min: {remaining_request}')\n",
    "\n",
    "        duration_ops = time.time() - start_time\n",
    "        duration_to_waiting = duration_min - duration_ops\n",
    "\n",
    "        if duration_to_waiting > 0:\n",
    "            time.sleep(duration_to_waiting)\n",
    "  \n",
    "\n",
    "    print('Producer: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def save_api_to_local(file_path:str, data: dict):\n",
    "    with open(file_path, 'w') as fp:\n",
    "        json.dump(data, fp)\n",
    "\n",
    "async def consumer(queue:asyncio.Queue, storage_path:Path):\n",
    "    print(\"Consumer: Started\")\n",
    "    while True:\n",
    "\n",
    "        response_body, id = await queue.get()\n",
    "        filename = Path(f'{id}.json')\n",
    "        file_path = storage_path / filename\n",
    "        print(f'Saving {id} at {file_path.as_posix()}')\n",
    "        await save_api_to_local(file_path=str(file_path), data=response_body)\n",
    "        queue.task_done()\n",
    "        print(f'Saving {id} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 0 in 0 batches with size (50)\n"
     ]
    }
   ],
   "source": [
    "# Configure log\n",
    "pass\n",
    "\n",
    "# Setup source\n",
    "source_base_path = Path().cwd().parent\n",
    "seed_path = source_base_path / Path('raw/seed/top-anime.csv')\n",
    "seeds = duckdb.read_csv(str(seed_path))\n",
    "sources = [row[0] for row in duckdb.sql('SELECT media_id FROM seeds').fetchall()]\n",
    "\n",
    "# Setup Target\n",
    "target_base_path = Path().cwd().parent\n",
    "storage_path = target_base_path / Path('raw/entity/anime-studio')\n",
    "targets = [int(filename.name.replace('.json', '')) for filename in storage_path.glob('*.json')]\n",
    "\n",
    "# Setup Delta\n",
    "delta = set(sources) - set(targets)\n",
    "pl_run = pl.DataFrame()\n",
    "pl_run = pl_run.with_columns([\n",
    "    pl.Series(list(delta)).alias('delta'), \n",
    "    pl.lit(None).alias('status'),\n",
    "    pl.lit(None).alias('updated')\n",
    "])\n",
    "\n",
    "df_run = duckdb.sql('SELECT * FROM pl_run')\n",
    "\n",
    "# Configure Pipeline Run Batch Size\n",
    "BATCH_SIZE = 50\n",
    "start_pos = 0\n",
    "end_pos = df_run.shape[0]\n",
    "\n",
    "print(f\"Total items: {end_pos - start_pos} in {int((end_pos - start_pos) / BATCH_SIZE)} batches with size ({BATCH_SIZE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Producer Required Data\n",
    "MAX_API_PER_MIN = 90\n",
    "DURATION_PER_API_CALL = round(60 / MAX_API_PER_MIN, 1)\n",
    "\n",
    "url = 'https://graphql.anilist.co'\n",
    "template_path = Path(r'graphql-template\\get-anime-studio.graphql')\n",
    "with open(str(template_path), 'r') as fp:\n",
    "    template = fp.read()\n",
    "\n",
    "# Setup Consumer Required Data\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_pos in range(start_pos, end_pos, BATCH_SIZE):\n",
    "    batches_delta = duckdb.sql(f\"\"\"\n",
    "    SELECT delta \n",
    "        FROM df_run\n",
    "        LIMIT {BATCH_SIZE}\n",
    "        OFFSET {batch_pos}\n",
    "    \"\"\").fetchall()\n",
    "    ids = [_[0] for _ in batches_delta]\n",
    "\n",
    "    print(f\"Batch: {batch_pos}\")\n",
    "\n",
    "    # Do work here\n",
    "    queue = asyncio.Queue(BATCH_SIZE)\n",
    "    # start the consumer\n",
    "    _ = asyncio.create_task(consumer(queue, storage_path))\n",
    "    # start the producer and wait for it to finish\n",
    "    await asyncio.create_task(producer(queue, ids, url, template, DURATION_PER_API_CALL))\n",
    "    # wait for all items to be processed\n",
    "    await queue.join()\n",
    "\n",
    "    # Update run table\n",
    "    completion_dt = datetime.now()\n",
    "    srs_updated = [completion_dt for i in ids]\n",
    "    srs_status = ['Completed' for i in ids]\n",
    "    \n",
    "    pl_delta = pl.DataFrame()\n",
    "    pl_delta = pl_delta.with_columns([\n",
    "        pl.Series(ids).alias('delta'), \n",
    "        pl.Series(srs_status).alias('status'),\n",
    "        pl.Series(srs_updated).alias('updated')\n",
    "    ])\n",
    "    pl_run.update(pl_delta, on='delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
